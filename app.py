# -*- coding: utf-8 -*-
"""LDA_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yS4n07UGjwX9y0GzGG7x0IDFqUllegHN
"""

import streamlit as st
import numpy as np
import pandas as pd
import ast

import gensim as gs
from gensim import corpora
from gensim.models import CoherenceModel
from gensim.models import LdaModel
from datetime import datetime

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Streamlit 페이지 기본 설정
st.set_page_config(
    page_title="LDA 토픽 모델링 시각화",
    page_icon="📊",
    layout="wide",
    initial_sidebar_state="expanded",
)

# 페이지 제목 설정
st.title("TMNLP 경제 기사 분석")

# 사이드바 제목
st.sidebar.header("")

# --- 함수 정의 ---
@st.cache_data
def load_and_filter_data(file_path, start_date, end_date):
    try:
        # 데이터 불러오기
        tokenized_articles_df = pd.read_csv(file_path, encoding='utf-8-sig')

        # 'tokens' 컬럼 변환
        tokenized_articles_df['tokens'] = tokenized_articles_df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

        # 날짜 형변환
        tokenized_articles_df['date'] = pd.to_datetime(tokenized_articles_df['date'], format='%Y.%m.%d')

        # 지정된 기간 내 데이터 필터링
        filtered_df = tokenized_articles_df[(tokenized_articles_df['date'] >= pd.to_datetime(start_date)) & (tokenized_articles_df['date'] <= pd.to_datetime(end_date))]

        st.success(f"{start_date} ~ {end_date} 기간의 기사 {len(filtered_df)}개를 찾았습니다.")  # 성공 메시지 표시

        return filtered_df

    except FileNotFoundError:
        st.error(f"오류: 해당 파일 '{file_path}'을 찾을 수 없습니다. 파일 경로를 확인해주세요.")
        return None
    except Exception as e:
        st.error(f"오류: 데이터 로딩 및 필터링 중 오류 발생: {e}")
        return None

# 단어 사전 및 BoW 코퍼스 생성 함수
@st.cache_data
def create_dictionary_and_corpus(filtered_df):
    try:
        with st.spinner("단어 사전 및 BoW 코퍼스 생성 중..."):
            # 단어 사전 생성
            dictionary = corpora.Dictionary(filtered_df['tokens'])

            # BoW 코퍼스 생성
            corpus = [dictionary.doc2bow(tokens) for tokens in filtered_df['tokens']]

        return dictionary, corpus

    except Exception as e:
        st.error(f"오류: 단어 사전 및 BoW 코퍼스 생성 중 오류 발생: {e}")
        return None, None

# LDA 모델 학습 및 최적 토픽 수 결정 함수
def train_lda_model(corpus, dictionary, filtered_df):
    topic_range = range(2, 10)  # 토픽 개수 범위를 2~9로 고정
    try:
        with st.spinner("최적 토픽 개수 탐색 및 LDA 모델 학습 중..."):
            coherence_scores = []
            num_topics_list = list(topic_range) # 리스트로 변환하여 인덱스 접근 가능하도록 함

            # 진행률 표시바 초기화
            progress_bar = st.progress(0)
            progress_text = "응집도 계산 진행 중..."
            status_text = st.empty() # 상태 메시지를 표시할 빈 공간 확보

            for i, num_topics in enumerate(num_topics_list):
                # LDA 모델 학습
                lda_model = gs.models.ldamodel.LdaModel(
                    corpus=corpus,
                    id2word=dictionary,
                    num_topics=num_topics,
                    passes=15,
                    random_state=42
                )

                # 응집도 계산
                coherence_model = CoherenceModel(
                    model=lda_model,
                    texts=filtered_df['tokens'],
                    dictionary=dictionary,
                    coherence='c_v'
                )
                coherence_lda = coherence_model.get_coherence()
                coherence_scores.append(coherence_lda)

                # 진행률 표시바 업데이트
                progress_value = (i + 1) / len(num_topics_list)
                progress_bar.progress(progress_value, text=f"{progress_text} ({progress_value*100:.1f}%)")

            # 최적 토픽 개수 결정
            if coherence_scores:
                max_coherence = max(coherence_scores)
                optimal_topic_index = coherence_scores.index(max_coherence)
                best_topic_num = num_topics_list[optimal_topic_index]

                st.success(f"최적 토픽 수: {best_topic_num} (응집도: {max_coherence:.4f})") # 응집도 값도 함께 표시

            else:
                st.warning("응집도 점수를 계산할 수 없습니다.")
                return None, None

            # 최종 LDA 모델 학습 (최적 토픽 개수로)
            final_lda_model = gs.models.ldamodel.LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=best_topic_num, # 최적 토픽 개수 사용
                passes=15, # passes 값 조정 가능
                random_state=42
            )
            return best_topic_num, final_lda_model # 학습된 최종 모델 반환

    except Exception as e:
        st.error(f"오류: LDA 모델 학습 중 오류 발생: {e}")
        st.error(e) # 상세 오류 메시지 표시
        return None, None



# --- UI 요소 ---
st.header("분석 기간 설정") # 사이드바 -> 메인 화면
col1, col2 = st.columns(2) # 두 개의 컬럼으로 나누기

with col1:
    start_date = st.date_input("시작 날짜", value=datetime(2025, 2, 1))
with col2:
    end_date = st.date_input("종료 날짜", value=datetime(2025, 4, 30))

# 파일 경로 설정
file_path = './tokenized_articles.csv'

# 분석 실행 버튼
if st.button("분석 실행"):
    st.write("분석을 시작합니다...")

    # 데이터 로딩 및 필터링
    filtered_df = load_and_filter_data(file_path, start_date, end_date)

    if filtered_df is not None:
        # 단어 사전 및 BoW 코퍼스 생성
        dictionary, corpus = create_dictionary_and_corpus(filtered_df)

        if dictionary is not None and corpus is not None:
            # LDA 모델 학습 및 최적 토픽 수 결정
            lda_model = train_lda_model(corpus, dictionary, filtered_df)

            if lda_model is not None:
                st.write("LDA 모델 학습 완료!")

                # pyLDAvis 시각화
                try:
                    with st.spinner("pyLDAvis 시각화 준비 중..."):
                        vis = gensimvis.prepare(lda_model[1], corpus, dictionary) # lda_model 튜플의 두 번째 요소가 모델
                        st.success("pyLDAvis 시각화")
                    pyldavis_html = pyLDAvis.prepared_data_to_html(vis)
		            st.components.v1.html(pyldavis_html, width=None, height=800, scrolling=True) 

                except Exception as e:
                    st.error(f"오류: pyLDAvis 시각화 중 오류 발생: {e}")
                    st.error("pyLDAvis 시각화가 제대로 표시되지 않을 수 있습니다. (Colab 환경)")

