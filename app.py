# -*- coding: utf-8 -*-
"""LDA_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yS4n07UGjwX9y0GzGG7x0IDFqUllegHN
"""

import streamlit as st
import numpy as np
import pandas as pd
import ast

import gensim as gs
from gensim import corpora
from gensim.models import CoherenceModel
from gensim.models import LdaModel
from datetime import datetime

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

import matplotlib.pyplot as plt
from wordcloud import WordCloud

import openai
import os
from dotenv import load_dotenv

# 환경변수 로딩
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

from openai import OpenAI
client = OpenAI()


# Streamlit 페이지 기본 설정
st.set_page_config(
    page_title="LDA 토픽 모델링 시각화",
    page_icon="📊",
    layout="wide",
    initial_sidebar_state="expanded",
)

# 페이지 제목 설정
st.title("TMNLP 경제 기사 분석")

# 사이드바 제목
st.sidebar.header("")

# --- 함수 정의 ---
@st.cache_data
def load_and_filter_data(file_path, start_date, end_date):
    try:
        # 데이터 불러오기
        tokenized_articles_df = pd.read_csv(file_path, encoding='utf-8-sig')

        # 'tokens' 컬럼 변환
        tokenized_articles_df['tokens'] = tokenized_articles_df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

        # 날짜 형변환
        tokenized_articles_df['date'] = pd.to_datetime(tokenized_articles_df['date'], format='%Y.%m.%d')

        # 지정된 기간 내 데이터 필터링
        filtered_df = tokenized_articles_df[(tokenized_articles_df['date'] >= pd.to_datetime(start_date)) & (tokenized_articles_df['date'] <= pd.to_datetime(end_date))]

        st.success(f"{start_date} ~ {end_date} 기간의 기사 {len(filtered_df)}개를 찾았습니다.")  # 성공 메시지 표시

        return filtered_df

    except FileNotFoundError:
        st.error(f"오류: 해당 파일 '{file_path}'을 찾을 수 없습니다. 파일 경로를 확인해주세요.")
        return None
    except Exception as e:
        st.error(f"오류: 데이터 로딩 및 필터링 중 오류 발생: {e}")
        return None

# 단어 사전 및 BoW 코퍼스 생성 함수
@st.cache_data
def create_dictionary_and_corpus(filtered_df):
    try:
        with st.spinner("단어 사전 및 BoW 코퍼스 생성 중..."):
            # 단어 사전 생성
            dictionary = corpora.Dictionary(filtered_df['tokens'])

            # BoW 코퍼스 생성
            corpus = [dictionary.doc2bow(tokens) for tokens in filtered_df['tokens']]

        return dictionary, corpus

    except Exception as e:
        st.error(f"오류: 단어 사전 및 BoW 코퍼스 생성 중 오류 발생: {e}")
        return None, None

# LDA 모델 학습 함수
def build_lda_model(corpus, dictionary, num_topics, passes=15, random_state=42):
    return gs.models.ldamodel.LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        passes=passes,
        random_state=random_state
    )

# LDA 모델 학습 및 최적 토픽 수 결정 함수
def train_lda_model(corpus, dictionary, filtered_df):
    topic_range = range(2, 10)
    try:
        with st.spinner("최적 토픽 개수 탐색 및 LDA 모델 학습 중..."):
            coherence_scores = []
            num_topics_list = list(topic_range)

            progress_bar = st.progress(0)
            progress_text = "응집도 계산 진행 중..."
            status_text = st.empty()

            for i, num_topics in enumerate(num_topics_list):
                lda_model = build_lda_model(corpus, dictionary, num_topics)

                coherence_model = CoherenceModel(
                    model=lda_model,
                    texts=filtered_df['tokens'],
                    dictionary=dictionary,
                    coherence='c_v'
                )
                coherence_lda = coherence_model.get_coherence()
                coherence_scores.append(coherence_lda)

                progress_value = (i + 1) / len(num_topics_list)
                progress_bar.progress(progress_value, text=f"{progress_text} ({progress_value*100:.1f}%)")

            if coherence_scores:
                max_coherence = max(coherence_scores)
                optimal_topic_index = coherence_scores.index(max_coherence)
                best_topic_num = num_topics_list[optimal_topic_index]

                st.success(f"최적 토픽 수: {best_topic_num} (응집도: {max_coherence:.4f})")
            else:
                st.warning("응집도 점수를 계산할 수 없습니다.")
                return None, None

            final_lda_model = build_lda_model(corpus, dictionary, best_topic_num)
            return best_topic_num, final_lda_model

    except Exception as e:
        st.error(f"오류: LDA 모델 학습 중 오류 발생: {e}")
        return None, None


def generate_wordcloud(filtered_df, start_date, end_date):
    if not filtered_df.empty and 'tokens' in filtered_df.columns:
        with st.spinner("워드클라우드 생성 중..."):
            try:
                all_tokens = [token for tokens_list in filtered_df['tokens'].dropna().tolist() for token in tokens_list]
                text_for_wordcloud = ' '.join(all_tokens)
                font_path = './NanumBarunGothic.ttf'
                wordcloud = WordCloud(
                    font_path=font_path,
                    width=800, height=400, 
                    background_color='white', 
                    max_words=100, 
                    colormap='viridis'
                )
                wordcloud.generate(text_for_wordcloud)
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                ax.set_title(f"'{start_date.strftime('%Y.%m.%d')}' ~ '{end_date.strftime('%Y.%m.%d')}'", fontsize=14)
                plt.close(fig)
                return fig
            
            except Exception as e:
                st.error(f"오류: 워드클라우드 생성 중 오류 발생: {e}")
                st.error(e)
                return None
    else:
        st.warning("\n워드클라우드를 생성할 텍스트 데이터가 없습니다 (필터링된 데이터 없음).")
        return None

# GPT 요약 함수
def summarize_topics_with_gpt(_lda_model, topn=10, model_name="gpt-4o"):
    try:
        summaries = []

        for topic_id in range(_lda_model.num_topics):
            top_words = _lda_model.show_topic(topic_id, topn=topn)
            keyword_list = [word for word, _ in top_words]
            prompt = (
                f"다음은 경제 기사 분석 결과에서 추출된 토픽 키워드입니다:\n"
                f"{', '.join(keyword_list)}\n\n"
                "이 키워드를 기반으로 해당 토픽을 간결하고 직관적인 문장으로 설명해 주세요."
            )

            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "당신은 경제 기사 분석 전문가입니다."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0.7
            )
            summary = response.choices[0].message.content.strip()
            summaries.append((topic_id, summary))

        return summaries

    except Exception as e:
        st.error(f"GPT 요약 중 오류 발생: {e}")
        return []

# --- UI 요소 ---
st.header("분석 기간 설정 (2025년 2월~4월 분석만 가능합니다.)") # 사이드바 -> 메인 화면
col1, col2 = st.columns(2) # 두 개의 컬럼으로 나누기

with col1:
    start_date = st.date_input("시작 날짜", value=datetime(2025, 2, 1))
with col2:
    end_date = st.date_input("종료 날짜", value=datetime(2025, 4, 30))

# 파일 경로 설정
file_path = './tokenized_articles.csv'

if 'lda_vis_html' not in st.session_state:
    st.session_state.lda_vis_html = None
if 'wordcloud_fig' not in st.session_state:
    st.session_state.wordcloud_fig = None
if 'topic_summaries' not in st.session_state:
    st.session_state.topic_summaries = None
    
# 시각화 선택 라디오 버튼
visualization_option = st.radio(
    "RESULT: ",
    ('LDA', 'wordCloud', 'summary')
)
    
if st.button("분석 실행"):
    filtered_df = load_and_filter_data(file_path, start_date, end_date)

    if filtered_df is not None:
        dictionary, corpus = create_dictionary_and_corpus(filtered_df)

        st.session_state.lda_vis_html = None
        if dictionary is not None and corpus is not None:
            lda_model = train_lda_model(corpus, dictionary, filtered_df)
            if lda_model is not None:
                try:
                    with st.spinner("pyLDAvis 시각화 준비 중..."):
                        vis = gensimvis.prepare(lda_model[1], corpus, dictionary)
                        st.session_state.lda_vis_html = pyLDAvis.prepared_data_to_html(vis)
                except Exception as e:
                    st.error(f"오류: pyLDAvis 시각화 중 오류 발생: {e}")
                    st.session_state.lda_vis_html = None
                    
            # GPT 요약 실행
            st.info("GPT 요약 생성 중...")
            topic_summaries = summarize_topics_with_gpt(lda_model[1])
            st.session_state.topic_summaries = topic_summaries
            st.success("GPT 요약 완료")

         # 워드클라우드 생성
        st.session_state.wordcloud_fig = None
        if not filtered_df.empty and 'tokens' in filtered_df.columns:
            st.info("워드클라우드 생성 중...")
            st.session_state.wordcloud_fig = generate_wordcloud(filtered_df, start_date, end_date)
            if st.session_state.wordcloud_fig:
                st.success("워드클라우드 생성 완료!")

# 결과만 번갈아 보여주기
if visualization_option == 'LDA':
    if st.session_state.lda_vis_html:
        st.components.v1.html(st.session_state.lda_vis_html, width=None, height=800, scrolling=True)
    else:
        st.info("LDA 결과가 없습니다. 먼저 분석을 실행하세요.")
elif visualization_option == 'wordCloud':
    if st.session_state.wordcloud_fig:
        st.pyplot(st.session_state.wordcloud_fig)
    else:
        st.info("워드클라우드 결과가 없습니다. 먼저 분석을 실행하세요.")
elif visualization_option == 'summary':
    if st.session_state.topic_summaries:
        st.subheader("GPT 기반 토픽 요약")
        for topic_id, summary in st.session_state.topic_summaries:
            st.markdown(f"**토픽 {topic_id + 1}**: {summary}")
    else:
        st.info("요약 결과가 없습니다. 먼저 분석을 실행하세요.")
        
